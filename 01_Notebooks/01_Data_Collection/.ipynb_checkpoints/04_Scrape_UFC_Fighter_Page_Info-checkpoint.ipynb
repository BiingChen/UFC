{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Scraping Fighter Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get List of Urls for Each Page of Fighters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.ufc.com/fighter/Weight_Class/filterFighters?offset={offset}&fighterFilter=All'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "number_of_fighters = int(soup.find('div', {'class':'paginate-results'}).find_all('span', {'class':'row-count'})[1].text)\n",
    "fighters_per_page = 20\n",
    "pages = math.ceil(number_of_fighters/fighters_per_page)\n",
    "\n",
    "urls = []\n",
    "for i in range(pages):\n",
    "    offset = i*20\n",
    "    url = f'http://www.ufc.com/fighter/Weight_Class/filterFighters?offset={offset}&fighterFilter=All'\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping Page 0 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 5 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 10 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 15 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 20 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 25 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 30 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 35 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 40 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 45 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 50 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 55 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 60 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 65 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 70 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 75 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 80 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 85 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 90 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 95 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 100 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Scrapping Page 105 of 110...\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "list_of_fighter_dict = []\n",
    "\n",
    "for page_num, url in enumerate(urls):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    \n",
    "    # Get the list of fighters object from the page\n",
    "    fighters_html = soup.find('table', {'class':'fighter-listing'})\n",
    "    list_of_fighter_htmls = fighters_html.find_all('tr', {'class':'fighter'})\n",
    "    \n",
    "    if page_num%5==0:\n",
    "        print(f\"Scrapping Page {page_num } of {len(urls)}...\")\n",
    "    for fighter_html in list_of_fighter_htmls:\n",
    "        fighter_dict = {}\n",
    "        fighter_dict['fighter_slug'] = fighter_html.find('div', {'class':'fighter-info'}).find('a').attrs['href'][9:]\n",
    "        fighter_dict['fighter_name'] = re.sub('\\n', '',fighter_html.find('div', {'class':'fighter-info'}).find('a').text.strip())\n",
    "        \n",
    "        if len(fighter_html.find_all('div', {'class':'main-txt'})) == 3:\n",
    "            # Get wins, losses and ties\n",
    "            win_loss_html = fighter_html.find_all('div', {'class':'main-txt'})[0]\n",
    "            if win_loss_html.text:\n",
    "                fighter_dict['wins'] = int(re.match(\"(\\d*)-(\\d*)-(\\d*)\", win_loss_html.text).group(1))\n",
    "                fighter_dict['losses'] = int(re.match(\"(\\d*)-(\\d*)-(\\d*)\", win_loss_html.text).group(2))\n",
    "                fighter_dict['ties'] = int(re.match(\"(\\d*)-(\\d*)-(\\d*)\", win_loss_html.text).group(3))\n",
    "\n",
    "            # Get Height - problem - sometimes no height\n",
    "\n",
    "            height_html = fighter_html.find_all('div', {'class':'main-txt'})[1]\n",
    "\n",
    "            feet = int(re.match(\"(\\d*)' (\\d*)\", height_html.text).group(1))\n",
    "            inches = int(re.match(\"(\\d*)' (\\d*)\", height_html.text).group(2))\n",
    "            total_height = feet*12 + inches\n",
    "            fighter_dict['height'] = total_height\n",
    "\n",
    "            # Get Weight\n",
    "            weight_html = fighter_html.find_all('div', {'class':'main-txt'})[2]\n",
    "            fighter_dict['weight'] = int(re.match(\"(\\d*) lbs\", weight_html.text).group(1))\n",
    "\n",
    "        list_of_fighter_dict.append(fighter_dict)\n",
    "        time.sleep(1)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_of_fighter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2183"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_fighter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Fighter Slug Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../02_Data/01_Raw_Scraped_Data/Fighters_Slug.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each fighter in the list, go to their UFC page and pull down their info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_fighter_info(list_fighter_slugs):\n",
    "    list_fighter_info_dict = []\n",
    "    \n",
    "    for key, fighter_slug in enumerate(list_fighter_slugs):\n",
    "        if key%25==0:\n",
    "            print(f'{key}: Scraping {fighter_slug} at {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")}')\n",
    "        fighter_url = f'http://www.ufc.com/fighter/{fighter_slug}'\n",
    "        try:\n",
    "            response = requests.get(fighter_url)\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "            fighter_info_dict = get_fighter_info(soup, fighter_slug)\n",
    "            list_fighter_info_dict.append(fighter_info_dict)\n",
    "\n",
    "        except:\n",
    "            log_scraping_fights(fighter_slug)\n",
    "            \n",
    "        time.sleep(1)\n",
    "        \n",
    "    return list_fighter_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scraping_fights(fighter_slug):\n",
    "    print(f'error with {fighter_slug} at {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")}')\n",
    "    with open('fighter_info_error_log.csv', 'a+', newline='') as f:\n",
    "        f.write(f'\\nerror scraping {fighter_slug} at {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fighter_info(soup, fighter_slug):\n",
    "    fighter_info_dict = {}\n",
    "    for tr in soup.find('div', {'class':'fighter-info'}).find('table').find_all('tr'):\n",
    "        label = tr.find('td', {'class':'label'}).text\n",
    "        value = tr.find('td', {'class':'value'}).text\n",
    "\n",
    "        # Try to clean up the value if exists\n",
    "        try:\n",
    "            value = re.sub('\\n','', value)\n",
    "            value = re.sub('\\t','', value)\n",
    "        except:\n",
    "            print('something here??')\n",
    "            pass\n",
    "        fighter_info_dict[label] = value\n",
    "    fighter_info_dict['fighter_slug'] = fighter_slug\n",
    "    return fighter_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Scraping danny-Abbadi at 2018-07-10 22:43\n",
      "25: Scraping alfonso-Alcarez at 2018-07-10 22:44\n",
      "50: Scraping adlan-Amagov at 2018-07-10 22:45\n",
      "75: Scraping igor-Araujo at 2018-07-10 22:45\n",
      "100: Scraping mehdi-Baghdad at 2018-07-10 22:46\n",
      "125: Scraping shayna-Baszler at 2018-07-10 22:47\n",
      "150: Scraping matt-Bessette at 2018-07-10 22:48\n",
      "175: Scraping steve-Bosse at 2018-07-10 22:49\n",
      "200: Scraping damien-brown at 2018-07-10 22:50\n",
      "225: Scraping kevin-Burns at 2018-07-10 22:50\n",
      "250: Scraping carlos-Candelario at 2018-07-10 22:51\n",
      "275: Scraping duane-Cason at 2018-07-10 22:52\n",
      "300: Scraping john-Cholish at 2018-07-10 22:53\n",
      "325: Scraping mark-Coleman at 2018-07-10 22:54\n",
      "350: Scraping jc-cottrell at 2018-07-10 22:55\n",
      "375: Scraping patrick-Cummins at 2018-07-10 22:56\n",
      "error with w.wec.tv/lcdavis at 2018-07-10 22:57\n",
      "400: Scraping marcus-Davis at 2018-07-10 22:57\n",
      "425: Scraping royden-Demotta at 2018-07-10 22:57\n",
      "450: Scraping russell-Doane at 2018-07-10 22:58\n",
      "475: Scraping alexis-Dufresne at 2018-07-10 22:59\n",
      "500: Scraping te-Edwards at 2018-07-10 23:00\n",
      "525: Scraping nick-Ertl at 2018-07-10 23:01\n",
      "550: Scraping ron-Faircloth at 2018-07-10 23:01\n",
      "575: Scraping tait-Fletcher at 2018-07-10 23:02\n",
      "600: Scraping anthony-Fryklund at 2018-07-10 23:03\n",
      "625: Scraping pablo-Garza at 2018-07-10 23:05\n",
      "650: Scraping rick-glenn at 2018-07-10 23:06\n",
      "675: Scraping gerard-Gordeau at 2018-07-10 23:07\n",
      "700: Scraping matt-Grice at 2018-07-10 23:08\n",
      "725: Scraping horacio-gutierrez at 2018-07-10 23:09\n",
      "750: Scraping dan-Hardy at 2018-07-10 23:10\n",
      "775: Scraping david-Heath at 2018-07-10 23:11\n",
      "800: Scraping heath-Herring at 2018-07-10 23:12\n",
      "825: Scraping kurt-holobaugh at 2018-07-10 23:13\n",
      "850: Scraping mark-Hughes at 2018-07-10 23:13\n",
      "875: Scraping yoislandy-Izquierdo at 2018-07-10 23:14\n",
      "900: Scraping zak-Jensen at 2018-07-10 23:16\n",
      "925: Scraping jon-Jones at 2018-07-10 23:16\n",
      "950: Scraping masanori-Kanehara at 2018-07-10 23:18\n",
      "975: Scraping rustam-Khabilov at 2018-07-10 23:19\n",
      "1000: Scraping cheick-Kongo at 2018-07-10 23:20\n",
      "1025: Scraping tim-Lajcik at 2018-07-10 23:22\n",
      "1050: Scraping thanh-Le at 2018-07-10 23:23\n",
      "1075: Scraping john-Letters at 2018-07-10 23:24\n",
      "1100: Scraping artem-Lobov at 2018-07-10 23:25\n",
      "1125: Scraping jason-MacDonald at 2018-07-10 23:26\n",
      "1150: Scraping john-makdessi at 2018-07-10 23:27\n",
      "1175: Scraping bobby-Martin at 2018-07-10 23:28\n",
      "1200: Scraping jason-Maxwell at 2018-07-10 23:29\n",
      "1225: Scraping antonio-mckee at 2018-07-10 23:30\n",
      "1250: Scraping yaotzin-Meza at 2018-07-10 23:31\n",
      "1275: Scraping mario-Miranda at 2018-07-10 23:32\n",
      "1300: Scraping augusto-montano at 2018-07-10 23:33\n",
      "1325: Scraping margaret-Morgan at 2018-07-10 23:34\n",
      "1350: Scraping tom-Murphy at 2018-07-10 23:35\n",
      "1375: Scraping roy-Nelson at 2018-07-10 23:36\n",
      "1400: Scraping kyle-Noke at 2018-07-10 23:37\n",
      "1425: Scraping charles-Oliveira at 2018-07-10 23:38\n",
      "1450: Scraping tom-Owens at 2018-07-10 23:39\n",
      "1475: Scraping roland-Parli at 2018-07-10 23:40\n",
      "1500: Scraping cathal-pendred at 2018-07-10 23:41\n",
      "1525: Scraping anthony-Pettis at 2018-07-10 23:42\n",
      "1550: Scraping ruan-Potts at 2018-07-10 23:43\n",
      "1575: Scraping michael-quinones at 2018-07-10 23:44\n",
      "1600: Scraping karl-Reed at 2018-07-10 23:45\n",
      "1625: Scraping mike-Rio at 2018-07-10 23:46\n",
      "1650: Scraping carlos-Eduardo-Rocha at 2018-07-10 23:47\n",
      "1675: Scraping ben-Rothwell at 2018-07-10 23:49\n",
      "1700: Scraping hayato-Sakurai at 2018-07-10 23:50\n",
      "1725: Scraping jorge-Santiago at 2018-07-10 23:52\n",
      "1750: Scraping kerry-Schall at 2018-07-10 23:53\n",
      "1775: Scraping mike-Serr at 2018-07-10 23:54\n",
      "1800: Scraping antonio-Silva at 2018-07-10 23:58\n",
      "1825: Scraping dennis-Siver at 2018-07-10 23:59\n",
      "error with rameau-Sokoudjou at 2018-07-11 00:12\n",
      "1850: Scraping dmitry-Sosnovskiy at 2018-07-11 00:12\n",
      "1875: Scraping cristina-Stanciu at 2018-07-11 00:16\n",
      "1900: Scraping curtis-Stout at 2018-07-11 00:17\n",
      "1925: Scraping eugenio-Tadeu at 2018-07-11 00:18\n",
      "1950: Scraping tra-Telligman at 2018-07-11 00:19\n",
      "1975: Scraping darren-till at 2018-07-11 00:20\n",
      "2000: Scraping tor-Troeng at 2018-07-11 00:21\n",
      "2025: Scraping richie-vaculik at 2018-07-11 00:22\n",
      "2050: Scraping ernie-Verdicia at 2018-07-11 00:23\n",
      "2075: Scraping TJ-Waldburger-fighter at 2018-07-11 00:24\n",
      "2100: Scraping allan-Weickert at 2018-07-11 00:25\n",
      "2125: Scraping tedd-Williams at 2018-07-11 00:26\n",
      "2150: Scraping rani-Yahya at 2018-07-11 00:27\n",
      "2175: Scraping joao-Zeferino at 2018-07-11 00:28\n"
     ]
    }
   ],
   "source": [
    "list_fighter_info_dict = get_all_fighter_info(df.fighter_slug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_fighter_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../02_Data/01_Raw_Scraped_Data/Fighter_Info2181.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
