{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "import pickle\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../02_Data/02_Processed_Data/X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "    \n",
    "with open('../../02_Data/02_Processed_Data/y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)    \n",
    "\n",
    "with open('../../02_Data/02_Processed_Data/X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "    \n",
    "with open('../../02_Data/02_Processed_Data/y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5129437869822485\n",
      "Test: 0.4823529411764706\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "print('Train:', lr.score(X_train,y_train))\n",
    "print('Test:', lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the untuned logistic regression again just as a reference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Lasso Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l1', max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "print('Train:', lr.score(X_train,y_train))\n",
    "print('Test:', lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can't get this cell to run successfully.  I believe there is an issue with too many features.  I've allowed this to run for over an hour with my CPU at 99.9% before interrupting the kernel and then running more models with feature selection\n",
    "\n",
    "I expect this to do better because I have a lot of features for the amount of data that I have.  Reducing the C value from 1 to 0.1 increases the regularization strength and changing the penalty from 'l2' to 'l1' changes the regularization from ridge (reducing coefficients) to lasso (completely dropping coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_params = {\n",
    "    'penalty' : ['l1','l2'],\n",
    "    'C':[0.001, 0.01, 0.1],\n",
    "    'max_iter':[1000]\n",
    "}\n",
    "gs = GridSearchCV(lr, param_grid=lr_params)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Test:',gs.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set up this model, but it won't run either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try modeling using 100 Best Parameters and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Select 100 Best and PCA datasets for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../02_Data/02_Processed_Data/X_train_100b.pkl', 'rb') as f:\n",
    "    X_train_100b = pickle.load(f)\n",
    "    \n",
    "with open('../../02_Data/02_Processed_Data/X_test_100b.pkl', 'rb') as f:\n",
    "    X_test_100b = pickle.load(f)\n",
    "    \n",
    "with open('../../02_Data/02_Processed_Data/X_train_100b_pca.pkl', 'rb') as f:\n",
    "    X_train_100b_pca = pickle.load(f)\n",
    "    \n",
    "with open('../../02_Data/02_Processed_Data/X_test_100b_pca.pkl', 'rb') as f:\n",
    "    X_test_100b_pca = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Logistic Regression using Select K Best 100 parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6168639053254438\n",
      "Test: 0.5686274509803921\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_100b, y_train)\n",
    "print('Train:', lr.score(X_train_100b,y_train))\n",
    "print('Test:', lr.score(X_test_100b,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By just selecting the top 100 best parameters, the untuned logistic regression improves 0.08 for the test set.  This is a nice improvement for just selecting the top 100 best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Lasso Regularization using Select K=100 Best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.603180473372781\n",
      "Test: 0.5627450980392157\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1', C=0.1, max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_100b, y_train)\n",
    "print('Train:', lr.score(X_train_100b,y_train))\n",
    "print('Test:', lr.score(X_test_100b,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement by adding Lasson Regularization.  This is not too surprising since I've already done feature selection through Selecting 100 Best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with GridSearch using Select K=100 Best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5968934911242604\n",
      "Best Parameters: {'C': 0.001, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "Train: 0.5998520710059172\n",
      "Test: 0.5803921568627451\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr_params = {\n",
    "    'penalty' : ['l1','l2'],\n",
    "    'C':[0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter':[1000, 2000]\n",
    "}\n",
    "gs = GridSearchCV(lr, param_grid=lr_params)\n",
    "gs.fit(X_train_100b, y_train)\n",
    "print('Best Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Train:',gs.score(X_train_100b,y_train))\n",
    "print('Test:',gs.score(X_test_100b,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch improved the best score by another 0.02 getting to 0.58.  I ran this gridsearch a number of times, adding additional search parameters and it ended with using 0.001 learning rate for ridge regression at a max iteration of 1000.  Selection of ridge over lasso regularization is not surprising since we've already done manual feature selection, so controlling for the size of coefficients is more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the best coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1_reach_adv</th>\n",
       "      <td>0.044867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_grappling_submissions_attempts_avg_diff</th>\n",
       "      <td>0.044572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_knock_down_landed_avg_diff</th>\n",
       "      <td>0.041376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2_total_strikes_landed_avg_diff</th>\n",
       "      <td>-0.022935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2_head_total_strikes_landed_avg_diff</th>\n",
       "      <td>-0.023089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_f2_clinch_head_strikes_landed_avg</th>\n",
       "      <td>-0.038542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "f1_reach_adv                                0.044867\n",
       "f1_grappling_submissions_attempts_avg_diff  0.044572\n",
       "f1_knock_down_landed_avg_diff               0.041376\n",
       "f2_total_strikes_landed_avg_diff           -0.022935\n",
       "f2_head_total_strikes_landed_avg_diff      -0.023089\n",
       "f1_f2_clinch_head_strikes_landed_avg       -0.038542"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = pd.DataFrame(gs.best_estimator_.coef_, columns=X_train_100b.columns).T.sort_values(0, ascending=False)\n",
    "top_bot_coefs = coefs.head(3)\n",
    "top_bot_coefs = top_bot_coefs.append(coefs.tail(3))\n",
    "top_bot_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reach advantage was the strongest predictor.  Based on my reading, this was expected.  Interestingly, the number of submission attempts was also a strong predictor of wins.  This is probably capturing the wins for submission fighters.  The 3rd best predictor was the difference in average knock downs.  This is an indicator for power punching, essentially saying if the fighter knocks down their past opponents more, then they have a higher likelihood of winning.\n",
    "\n",
    "For negative predictors, or predictors of losses, the top score of f1_f2 clinch head strikes is confusing.  f1_f2 in the data indicates the inverse \"defensive\" rating.  A high f1_f2 value would indicate that fighter 1 is bad at avoiding these types of strikes.  Essentially, this is saying that if a fighter is bad at avoiding clinch head strikes, they will more likely lose.  The other 2 negative predictors make sense.  The difference in average head total strikes and total strikes for fighter 2.  Basically if the opponent has high strikes landed in their past fights, it means that fighter 1 has a higher chance of losing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5857777777777777\n",
      "Best Parameters: {'C': 0.01, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "Train: 0.6068786982248521\n",
      "Test: 0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr_params = {\n",
    "    'penalty' : ['l1','l2'],\n",
    "    'C':[0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter':[1000, 2000]\n",
    "}\n",
    "time_cv = TimeSeriesSplit(n_splits=5).split(X_train_100b)\n",
    "gs = GridSearchCV(lr, param_grid=lr_params, cv=time_cv, )\n",
    "gs.fit(X_train_100b, y_train)\n",
    "print('Best Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Train:',gs.score(X_train_100b,y_train))\n",
    "print('Test:',gs.score(X_test_100b,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if the time series split is actually working correctly.  It looks like the time series split is just splitting up the data by index...  I need to try to sort "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using Select K Best 100 parameters and then PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5795118343195266\n",
      "Test: 0.5705882352941176\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_100b_pca, y_train)\n",
    "print('Train:', lr.score(X_train_100b_pca,y_train))\n",
    "print('Test:', lr.score(X_test_100b_pca,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is very little overfitting happening now even with an untuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Lasso using Select K=100 Best parameters and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5821005917159763\n",
      "Test: 0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1', C=0.1, max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_100b_pca, y_train)\n",
    "print('Train:', lr.score(X_train_100b_pca,y_train))\n",
    "print('Test:', lr.score(X_test_100b_pca,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, unsurprisingly logistic regression with lasso regularization doesn't do much on data that has already had feature selection/reduction performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with GridSearch using Select K=100 Best parameters and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.584689349112426\n",
      "Best Parameters: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1'}\n",
      "Train: 0.5835798816568047\n",
      "Test: 0.5705882352941176\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_params = {\n",
    "    'penalty' : ['l1','l2'],\n",
    "    'C':[0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter':[1000, 2000]\n",
    "}\n",
    "gs = GridSearchCV(lr, param_grid=lr_params)\n",
    "gs.fit(X_train_100b_pca, y_train)\n",
    "print('Best Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Train:',gs.score(X_train_100b_pca,y_train))\n",
    "print('Test:',gs.score(X_test_100b_pca,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly the gridsearch using 100 best parameters and PCA picked lasso regression.  The overall score didn't change much topping out at 0.57"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
