{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Transform V2 Fight Details to add Percentages and Differentials\n",
    "- Calculate historical average of fight stats for each fight\n",
    "- Everything is from the perspective of the F1 fighter\n",
    "- Stats for the F1 fighter are basically an \"offensive\" rating for the F1 fighter\n",
    "- Stats for the F2 fighter are basically an inverse \"defensive\" rating for the F1 fighter\n",
    "- We also want to look at the difference in the stats for each fight, and then historical average difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in V2_Fight_Details CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../02_Data/02_Processed_Data/V2_Fight_Details.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on this initial set of data:\n",
    "- No nulls currently since I dropped them all already\n",
    "- No time in position details since they had nulls\n",
    "- Only 1 side.  Not flipped and appended yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to add and flip before the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Columns Again\n",
    "F1_Columns = [col for col in df.columns if 'f1' in col.lower()]\n",
    "F2_Columns = [col for col in df.columns if 'f2' in col.lower()]\n",
    "Other_Columns = [col for col in df.columns if not 'f2' in col.lower() and not 'f1' in col.lower()]\n",
    "\n",
    "Ordered_Columns = Other_Columns + F1_Columns + F2_Columns\n",
    "Flipped_Columns = Other_Columns + F2_Columns + F1_Columns\n",
    "\n",
    "# Put Columns in Order\n",
    "df = df[Ordered_Columns]\n",
    "\n",
    "# Create Flipped df\n",
    "flipped_df = df[Flipped_Columns]\n",
    "flipped_df.columns = Ordered_Columns\n",
    "\n",
    "# Concatenate df and flipped_df\n",
    "df = pd.concat([df, flipped_df])\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Column References for Easy Access  *Do not delete*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns are used in the transformations\n",
    "F1_Columns = [col for col in df.columns if 'f1' in col.lower()]\n",
    "F2_Columns = [col for col in df.columns if 'f2' in col.lower()]\n",
    "Other_Columns = [col for col in df.columns if not 'f2' in col.lower() and not 'f1' in col.lower()]\n",
    "\n",
    "F1_Strikes = [col for col in F1_Columns if 'strikes' in col.lower()] + ['F1_Knock_Down_Landed']\n",
    "F1_Grappling = [col for col in F1_Columns if 'grappling' in col.lower()]\n",
    "F1_TIP = [col for col in F1_Columns if 'tip' in col.lower()]\n",
    "F1_Identification = ['F1_FighterID','F1_Name'] # This is what its supposed to be incase I add more columns\n",
    "F1_Identification = list(set(F1_Columns) - set(F1_Strikes) - set(F1_Grappling) - set(F1_TIP))\n",
    "\n",
    "F2_Strikes = [col for col in F2_Columns if 'strikes' in col.lower()] + ['F2_Knock_Down_Landed']\n",
    "F2_Grappling = [col for col in F2_Columns if 'grappling' in col.lower()]\n",
    "F2_TIP = [col for col in F2_Columns if 'tip' in col.lower()]\n",
    "F2_Identification = ['F2_FighterID','F2_Name']\n",
    "F2_Identification = list(set(F2_Columns) - set(F2_Strikes) - set(F2_Grappling) - set(F2_TIP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Transformation:  Calculate success percent for every pair of words that have \"landed\" and \"attempted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perform_calc_success_percent(df):\n",
    "    Both_Landed_Attempts, Attempts_Only, Landed_Only = get_attempts_landed_columns(df)\n",
    "    num_columns_before = df.shape[1]\n",
    "    columns_before = df.columns\n",
    "    print(f'num columns before: {num_columns_before}')\n",
    "    for col in Both_Landed_Attempts:\n",
    "        df = calc_success_percent(df, col)\n",
    "    num_columns_after = df.shape[1]\n",
    "    columns_after = df.columns\n",
    "    print(f\"# Columns Added: {num_columns_after-num_columns_before}\")\n",
    "    print(f'num columns after: {num_columns_after}')\n",
    "    new_columns = list(set(columns_after) - set(columns_before))\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_success_percent(df, col_root):\n",
    "    attempts = col_root + '_attempts'\n",
    "    landed = col_root + '_landed'\n",
    "    success_percent = col_root + '_percent'\n",
    "    \n",
    "    df[success_percent] = df.apply(lambda x: x[landed]/x[attempts] if x[attempts] != 0 else 0, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#calc_success_percent(df, 'F1_Body_Significant_Strikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attempts_landed_columns(df):\n",
    "    list_cols_attempts = []\n",
    "    list_cols_landed = []\n",
    "    for column in df.columns: \n",
    "        if '_attempts' in column.lower():\n",
    "            list_cols_attempts.append(column)\n",
    "        if '_landed' in column.lower():\n",
    "            list_cols_landed.append(column)\n",
    "    list_cols_attempts = [re.sub('_attempts','',col) for col in list_cols_attempts] # Remove \"_Attempts\"\n",
    "    list_cols_landed = [re.sub('_landed','',col) for col in list_cols_landed]\n",
    "    Attempts_Only = set(list_cols_attempts) - set(list_cols_landed)\n",
    "    Landed_Only =  set(list_cols_landed) - set(list_cols_attempts)\n",
    "    Both_Landed_Attempts = set(list_cols_landed) & set(list_cols_attempts)\n",
    "    return Both_Landed_Attempts, Attempts_Only, Landed_Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num columns before: 107\n",
      "# Columns Added: 46\n",
      "num columns after: 153\n"
     ]
    }
   ],
   "source": [
    "df, Landed_Percent = Perform_calc_success_percent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Transformation:  Calculate In Fight Differentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_calc_columns = list(set(F1_Columns) - set(F1_Identification) - set(F1_TIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also need to get the percent columns\n",
    "# At this point the df already has the percent columns... so just grab them from the df directly\n",
    "percent_cols = [col[3:] for col in df.columns if 'percent' in col and 'f1' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perform_calc_fight_stat_differential(df):\n",
    "    percent_cols = [col[3:] for col in df.columns if 'percent' in col and 'f1' in col]\n",
    "    f1_calc_columns = list(set(F1_Columns) - set(F1_Identification) - set(F1_TIP))\n",
    "    for_calc_columns = [col[3:] for col in f1_calc_columns] # Remove the F1_ part\n",
    "    for_calc_columns.extend(percent_cols)\n",
    "    num_columns_before = df.shape[1]\n",
    "    columns_before = df.columns\n",
    "    print(f'num columns before: {num_columns_before}')\n",
    "    for col in for_calc_columns:\n",
    "        df = calc_fight_stat_differential(df, col)\n",
    "    num_columns_after = df.shape[1]\n",
    "    columns_after = df.columns\n",
    "    print(f\"# Columns Added: {num_columns_after-num_columns_before}\")\n",
    "    print(f'num columns after: {num_columns_after}')\n",
    "    new_columns = list(set(columns_after) - set(columns_before))\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fight_stat_differential(df, col_root):\n",
    "    f1 = 'f1_' + col_root\n",
    "    f2 = 'f2_' + col_root\n",
    "    diff = col_root + '_diff'\n",
    "    df[diff] = df.apply(lambda x: (x[f1] - x[f2])/(x[f1] + x[f2]) if (x[f1] + x[f2]) != 0 else 0, axis=1)\n",
    "    return df\n",
    "\n",
    "# calc_fight_stat_differential(df, col_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num columns before: 153\n",
      "# Columns Added: 71\n",
      "num columns after: 224\n"
     ]
    }
   ],
   "source": [
    "df, stat_diff = Perform_calc_fight_stat_differential(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This dataset is currently missing the date.  Join it from the Events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../02_Data/01_Raw_Scraped_Data/Events/events_df.csv'\n",
    "\n",
    "events_df = pd.read_csv(path, index_col=0)\n",
    "events_df = events_df[['EventId','Date']]\n",
    "events_df = events_df.rename(index=str, columns={\"EventId\": \"eventid\", 'Date':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(events_df, on='eventid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4774, 225)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing for V2 Df is done.  Export it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../02_Data/02_Processed_Data/V2_Processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_DF.csv                       df_ems.csv\r\n",
      "V1_DF_w_flipped.csv             fighter_page_info.csv\r\n",
      "V2_Fight_Details.csv            fighter_static_stats.csv\r\n",
      "V2_Fight_Details_Munged0711.csv train.csv\r\n",
      "V2_Processed.csv                train_stub.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../02_Data/02_Processed_Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Stub Data\n",
    "train_stub = ['eventid','fightid','f1_fullname','f2_fullname','f1_fighterid','f2_fighterid','f1_outcome']\n",
    "train = V1_df[train_stub].reset_index().drop(columns='index')\n",
    "\n",
    "\n",
    "df_ems_f1 = df_ems.copy()\n",
    "\n",
    "# First append 'F1_' for all fighter 1 data\n",
    "df_ems_f1.columns = ['eventid','fightid','f1_fighterid','date'] + \\\n",
    "                    ['f1_' + col for col in df_ems_f1.columns if col not in \\\n",
    "                    ['eventid','fightid','f1_fighterid','date']]\n",
    "\n",
    "# Merge F1 Expanding Means\n",
    "train = train.merge(df_ems_f1, left_on=['eventid','fightid','f1_fighterid'],\n",
    "                    right_on=['eventid','fightid','f1_fighterid'])\n",
    "\n",
    "#Setup \n",
    "df_ems_f2 = df_ems.drop(columns=['date']).copy()\n",
    "df_ems_f2.columns = ['eventid','fightid','f2_fighterid'] + \\\n",
    "                    ['f2_' + col for col in df_ems_f2.columns if col not in \\\n",
    "                    ['eventid','fightid','f1_fighterid']]\n",
    "\n",
    "# Merge em for fighter 2\n",
    "train = train.merge(df_ems_f2, left_on=['eventid','fightid','f2_fighterid'], \n",
    "                    right_on=['eventid','fightid','f2_fighterid'])\n",
    "\n",
    "# drop columns w/o outcome\n",
    "train = train.dropna(axis=0, how='any')\n",
    "\n",
    "# Check empty columns:\n",
    "check_nulls(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to clean up data a bit more before I can model\n",
    "- Label from win/loss to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Win     1590\n",
       "Loss    1590\n",
       "Name: f1_outcome, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.f1_outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['outcome'] = train.f1_outcome.map(lambda x: 1 if x=='Win' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Train dataset for some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../../02_Data/02_Processed_Data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the data is \"prepped\" and I can try to train test split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = set(train.f1_fighterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = list(set(train.f1_fighterid))\n",
    "test_full = train[0:0]\n",
    "train_full = train[0:0]\n",
    "\n",
    "\n",
    "for fighter_id in unique_ids:\n",
    "    sub_df = train[train.f1_fighterid == fighter_id].sort_values('date', ascending=False)\n",
    "#     print(sub_df.shape[0])\n",
    "#     print(fighter_id)\n",
    "    if sub_df.shape[0] > 6:\n",
    "        test_records = sub_df[:2]\n",
    "        train_records = sub_df[2:]\n",
    "        test_full = pd.concat([test_full, test_records], axis=0)\n",
    "        train_full = pd.concat([train_full, train_records], axis=0)\n",
    "    elif sub_df.shape[0] > 3:\n",
    "        test_records = sub_df[:1]\n",
    "        train_records = sub_df[1:]\n",
    "        test_full = pd.concat([test_full, test_records], axis=0)\n",
    "        train_full = pd.concat([train_full, train_records], axis=0)\n",
    "    else:\n",
    "        train_records = sub_df\n",
    "        train_full = pd.concat([train_full, train_records], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18745332337565349"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(test_full)/len(train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split out X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'outcome'\n",
    "features = [col for col in train_full.columns if col != label]\n",
    "y_train = train_full[label]\n",
    "X_train = train_full[features]\n",
    "\n",
    "# Drop non-number features\n",
    "X_train = X_train.select_dtypes(exclude='object')\n",
    "# Drop non-feature columns\n",
    "X_train = X_train.drop(columns=['eventid','fightid','f1_fighterid','f2_fighterid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2678, 384)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'outcome'\n",
    "features = [col for col in test_full.columns if col != label]\n",
    "y_test = test_full[label]\n",
    "X_test = test_full[features]\n",
    "\n",
    "# Drop non-number features\n",
    "X_test = X_test.select_dtypes(exclude='object')\n",
    "# Drop non-feature columns\n",
    "X_test = X_test.drop(columns=['eventid','fightid','f1_fighterid','f2_fighterid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a quick and dirty model and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6631814787154593\n",
      "Test: 0.5677290836653387\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print('Train:', lr.score(X_train,y_train))\n",
    "print('Test:', lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9873039581777446\n",
      "Test: 0.547808764940239\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "print('Train:', rf.score(X_train,y_train))\n",
    "print('Test:',rf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5418222554144885\n",
      "Best Parameters: {}\n",
      "Test: 0.5597609561752988\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(random_state=42)\n",
    "ada_params = {}\n",
    "gs = GridSearchCV(ada, param_grid=ada_params)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Test:',gs.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] learning_rate=0.05, max_depth=3 .................................\n",
      "[CV] learning_rate=0.05, max_depth=3 .................................\n",
      "[CV] learning_rate=0.05, max_depth=3 .................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=3, total=   3.0s\n",
      "[CV] learning_rate=0.05, max_depth=4 .................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=3, total=   3.1s\n",
      "[CV] learning_rate=0.05, max_depth=4 .................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=3, total=   3.2s\n",
      "[CV] learning_rate=0.05, max_depth=4 .................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=4, total=   4.7s\n",
      "[CV] learning_rate=0.05, max_depth=5 .................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=4, total=   4.6s\n",
      "[CV] learning_rate=0.05, max_depth=5 .................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=4, total=   4.8s\n",
      "[CV] learning_rate=0.05, max_depth=5 .................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=5, total=   6.7s\n",
      "[CV] learning_rate=0.1, max_depth=3 ..................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=5, total=   6.6s\n",
      "[CV] learning_rate=0.1, max_depth=3 ..................................\n",
      "[CV] .................. learning_rate=0.05, max_depth=5, total=   6.8s\n",
      "[CV] learning_rate=0.1, max_depth=3 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=3, total=   3.1s\n",
      "[CV] learning_rate=0.1, max_depth=4 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=3, total=   3.0s\n",
      "[CV] learning_rate=0.1, max_depth=4 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=3, total=   3.1s\n",
      "[CV] learning_rate=0.1, max_depth=4 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=4, total=   4.5s\n",
      "[CV] learning_rate=0.1, max_depth=5 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=4, total=   4.5s\n",
      "[CV] learning_rate=0.1, max_depth=5 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=4, total=   4.7s\n",
      "[CV] learning_rate=0.1, max_depth=5 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=5, total=   6.6s\n",
      "[CV] learning_rate=0.3, max_depth=3 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=5, total=   6.7s\n",
      "[CV] learning_rate=0.3, max_depth=3 ..................................\n",
      "[CV] ................... learning_rate=0.1, max_depth=5, total=   6.7s\n",
      "[CV] learning_rate=0.3, max_depth=3 ..................................\n",
      "[CV] ................... learning_rate=0.3, max_depth=3, total=   3.1s\n",
      "[CV] learning_rate=0.3, max_depth=4 ..................................\n",
      "[CV] ................... learning_rate=0.3, max_depth=3, total=   3.1s\n",
      "[CV] learning_rate=0.3, max_depth=4 ..................................\n",
      "[CV] ................... learning_rate=0.3, max_depth=3, total=   3.0s\n",
      "[CV] learning_rate=0.3, max_depth=4 ..................................\n",
      "[CV] ................... learning_rate=0.3, max_depth=4, total=   4.7s\n",
      "[CV] learning_rate=0.3, max_depth=5 ..................................\n",
      "[CV] ................... learning_rate=0.3, max_depth=4, total=   4.6s\n",
      "[CV] learning_rate=0.3, max_depth=5 ..................................\n",
      "[CV] ................... learning_rate=0.3, max_depth=4, total=   4.7s\n",
      "[CV] learning_rate=0.3, max_depth=5 ..................................\n",
      "[CV] ................... learning_rate=0.3, max_depth=5, total=   6.6s\n",
      "[CV] ................... learning_rate=0.3, max_depth=5, total=   6.5s\n",
      "[CV] ................... learning_rate=0.3, max_depth=5, total=   6.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  27 out of  27 | elapsed:   43.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.1 s, sys: 99.8 ms, total: 7.2 s\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb_params = {\n",
    "    'learning_rate': [0.05, 0.1, 0.3],\n",
    "    'max_depth': [3,4,5]\n",
    "}\n",
    "gb_gs = GridSearchCV(gb, param_grid=gb_params, verbose=2, n_jobs=3 )\n",
    "gb_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'learning_rate': 0.05, 'max_depth': 4}\n",
      "Train: 0.9213528932355338\n",
      "Test: 0.5772727272727273\n"
     ]
    }
   ],
   "source": [
    "print('Best Params:', gb_gs.best_params_)\n",
    "print('Train:', gb_gs.score(X_train,y_train))\n",
    "print('Test:', gb_gs.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] learning_rate=0.5, n_estimators=50 ..............................\n",
      "[CV] learning_rate=0.5, n_estimators=50 ..............................\n",
      "[CV] learning_rate=0.5, n_estimators=50 ..............................\n",
      "[CV] ............... learning_rate=0.5, n_estimators=50, total=   2.1s\n",
      "[CV] ............... learning_rate=0.5, n_estimators=50, total=   2.1s\n",
      "[CV] learning_rate=0.5, n_estimators=100 .............................\n",
      "[CV] ............... learning_rate=0.5, n_estimators=50, total=   2.1s\n",
      "[CV] learning_rate=0.5, n_estimators=100 .............................\n",
      "[CV] learning_rate=0.5, n_estimators=100 .............................\n",
      "[CV] .............. learning_rate=0.5, n_estimators=100, total=   4.2s\n",
      "[CV] learning_rate=1.0, n_estimators=50 ..............................\n",
      "[CV] .............. learning_rate=0.5, n_estimators=100, total=   4.3s\n",
      "[CV] learning_rate=1.0, n_estimators=50 ..............................\n",
      "[CV] .............. learning_rate=0.5, n_estimators=100, total=   4.3s\n",
      "[CV] learning_rate=1.0, n_estimators=50 ..............................\n",
      "[CV] ............... learning_rate=1.0, n_estimators=50, total=   2.1s\n",
      "[CV] learning_rate=1.0, n_estimators=100 .............................\n",
      "[CV] ............... learning_rate=1.0, n_estimators=50, total=   2.1s\n",
      "[CV] ............... learning_rate=1.0, n_estimators=50, total=   2.1s\n",
      "[CV] learning_rate=1.0, n_estimators=100 .............................\n",
      "[CV] learning_rate=1.0, n_estimators=100 .............................\n",
      "[CV] .............. learning_rate=1.0, n_estimators=100, total=   4.2s\n",
      "[CV] .............. learning_rate=1.0, n_estimators=100, total=   4.2s\n",
      "[CV] .............. learning_rate=1.0, n_estimators=100, total=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 out of  12 | elapsed:   13.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.31 s, sys: 67.1 ms, total: 3.38 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "ada_params = {\n",
    "    'n_estimators':[50,100],\n",
    "    'learning_rate': [0.5, 1.0]\n",
    "}\n",
    "ada_gs = GridSearchCV(ada, param_grid=ada_params, verbose=2, n_jobs=3)\n",
    "ada_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5418222554144885\n",
      "Best Parameters: {'learning_rate': 1.0, 'n_estimators': 50}\n",
      "Train: 0.6747572815533981\n",
      "Test: 0.5597609561752988\n"
     ]
    }
   ],
   "source": [
    "print('Best Score:', ada_gs.best_score_)\n",
    "print('Best Parameters:', ada_gs.best_params_)\n",
    "print('Train:',ada_gs.score(X_train,y_train))\n",
    "print('Test:',ada_gs.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
